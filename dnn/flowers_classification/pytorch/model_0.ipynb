{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 14:20:18.702065: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-14 14:20:18.742683: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-14 14:20:18.742719: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-14 14:20:18.743660: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-14 14:20:18.750108: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-14 14:20:19.491174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from utils import print_and_get_nb_trainable_parameters, print_trainable_parameters\n",
    "import os\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    data_dir = Path('data')\n",
    "    train_dir = data_dir / 'train'\n",
    "    validation_dir = data_dir / 'validation'\n",
    "    image_shape = (3, 224, 224)\n",
    "    image_size = (224, 224)\n",
    "    num_workers = os.cpu_count()\n",
    "    batch_size = 32\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset using ImageFolder\n",
    "train_transforms = transforms.Compose([\n",
    "                                 transforms.Resize(config.image_size),\n",
    "                                 transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                 transforms.RandomVerticalFlip(p=0.5),\n",
    "                                #  transforms.RandomRotation(degrees=45),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
    "                                 ])\n",
    "validation_transforms = transforms.Compose([\n",
    "                                    transforms.Resize(config.image_size),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m validation_dataset \u001b[38;5;241m=\u001b[39m ImageFolder(config\u001b[38;5;241m.\u001b[39mvalidation_dir, transform\u001b[38;5;241m=\u001b[39mvalidation_transforms)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mlen\u001b[39m(train_dataset), \u001b[38;5;28mlen\u001b[39m(validation_dataset)\n",
      "File \u001b[0;32m~/anaconda3/envs/dnn/lib/python3.11/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/anaconda3/envs/dnn/lib/python3.11/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m~/anaconda3/envs/dnn/lib/python3.11/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dnn/lib/python3.11/site-packages/torchvision/datasets/folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train'"
     ]
    }
   ],
   "source": [
    "train_dataset = ImageFolder(config.train_dir, transform=train_transforms)\n",
    "validation_dataset = ImageFolder(config.validation_dir, transform=validation_transforms)\n",
    "len(train_dataset), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#create dataloaders\u001b[39;00m\n\u001b[1;32m      3\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m----> 4\u001b[0m         \u001b[43mtrain_dataset\u001b[49m,\n\u001b[1;32m      5\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m      6\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_workers,\n\u001b[1;32m      8\u001b[0m         pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     11\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     12\u001b[0m     validation_dataset,\n\u001b[1;32m     13\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#create dataloaders\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 66,565 || all params: 4,062,181 || trainable%: 1.638651748900406\n",
      "trainable params: 66,565 || all params: 4,062,181 || trainable%: 1.638651748900406\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, padding='same'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=5, padding='same'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding='same'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding='same'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding='same'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding='same'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding='same'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding='same'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        self.features.requires_grad_(False)\n",
    "\n",
    "        print_trainable_parameters(self)\n",
    "        print_and_get_nb_trainable_parameters(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "# config.class_names = train_dataset.classes\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_1(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, padding='same'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=5, padding='same'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size='same'),  # 1x1 convolution\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding='same'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=1),  # 1x1 convolution\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding='same'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        # self.features.requires_grad_(False)\n",
    "\n",
    "        # print_trainable_parameters(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "config.class_names = train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModel(keras.Model):\n",
    "    keras.backend.clear_session()\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = keras.layers.TorchModuleWrapper(Net(len(config.class_names)), name='torch_model')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "keras_model = KerasModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"keras_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"keras_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">    Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ torch_model                     │ ?                         │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,062,181</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TorchModuleWrapper</span>)            │                           │            │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ torch_model                     │ ?                         │  \u001b[38;5;34m4,062,181\u001b[0m │\n",
       "│ (\u001b[38;5;33mTorchModuleWrapper\u001b[0m)            │                           │            │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,062,181</span> (15.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,062,181\u001b[0m (15.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,062,181</span> (15.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,062,181\u001b[0m (15.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adamW\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.fit(\n",
    "    train_dataloader,\n",
    "    epochs=10,\n",
    "    validation_data=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def train_one_epoch(model: torch.nn.Module,\n",
    "                    train_dataloader: torch.utils.data.DataLoader,\n",
    "                    loss_fn,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    scheduler: torch.optim.lr_scheduler,\n",
    "                    device: torch.device,\n",
    "                    f1score: Callable,\n",
    "                    accuracy: Callable,\n",
    "                    DEBUG: bool = False) -> tuple:\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using the specified dataloader and returns the average training loss,\n",
    "    accuracy and F1-score for the epoch.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to be trained\n",
    "        epoch: The current epoch number\n",
    "        train_dataloader: DataLoader object containing the training data\n",
    "        loss_fn: The loss function to be used for optimization\n",
    "        optimizer: The optimization algorithm to be used\n",
    "        scheduler: The learning rate scheduler to be used\n",
    "        device: The device on which the computations should be carried out\n",
    "        f1score: The F1-score metric to be used for evaluation\n",
    "        accuracy: The accuracy metric to be used for evaluation\n",
    "        DEBUG: A boolean flag to enable debug mode\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: A tuple containing the average training loss, accuracy and F1-score for the epoch.\n",
    "    \"\"\"\n",
    "    # Set the model to training mode.\n",
    "    model.train()\n",
    "    # Initialize the running totals for the loss, accuracy and F1-score.\n",
    "    train_acc_total, train_loss_total, train_f1score_total = 0, 0, 0\n",
    "    # Iterate over the training dataloader.\n",
    "    for batch_idx, (X, y) in enumerate(train_dataloader):\n",
    "        # If DEBUG is enabled, break after the first 10 batches.\n",
    "        if DEBUG and batch_idx == 10:\n",
    "            break\n",
    "        # Move the data to the device.\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Forward pass.\n",
    "        y_pred = model(X)\n",
    "        # Calculate the loss.\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # Backward pass.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update the parameters.\n",
    "        optimizer.step()\n",
    "        # Update the learning rate scheduler.\n",
    "        scheduler.step()\n",
    "        # Calculate the accuracy and F1-score.\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        f1score_metrics = f1score(y_pred_class, y)\n",
    "        acc = accuracy(y_pred_class, y)\n",
    "        # Print a progress message.\n",
    "        msg = f\"Batch: {batch_idx + 1} | Loss: {loss.item():.4f} | Accuracy: {acc:.4f} | F1Score: {f1score_metrics:.4f}\"\n",
    "        print('\\r' + msg, end='', flush=True)\n",
    "        # Update the running totals.\n",
    "        train_loss_total = train_loss_total + loss.item()\n",
    "        train_acc_total = train_acc_total + acc\n",
    "        train_f1score_total = train_f1score_total + f1score_metrics\n",
    "    # Calculate the average loss, accuracy and F1-score.\n",
    "    train_loss_avg = train_loss_total / len(train_dataloader)\n",
    "    train_acc_avg = train_acc_total / len(train_dataloader)\n",
    "    train_f1score_avg = train_f1score_total / len(train_dataloader)\n",
    "    # If DEBUG is enabled, calculate the average loss, accuracy and F1-score for the first 10 batches.\n",
    "    if DEBUG:\n",
    "        train_loss_avg = train_loss_total / 10\n",
    "        train_acc_avg = train_acc_total / 10\n",
    "        train_f1score_avg = train_f1score_total / 10\n",
    "    # Print the epoch history.\n",
    "    epoch_history = f\"Loss: {train_loss_avg:.4f} | Accuracy: {train_acc_avg:.4f} | F1Score: {train_f1score_avg:.4f}\"\n",
    "    print('\\r' + epoch_history, end='\\n', flush=True)\n",
    "    return train_loss_avg, train_acc_avg, train_f1score_avg\n",
    "\n",
    "\n",
    "# function to validate the model for one epoch\n",
    "def validate_one_epoch(model: torch.nn.Module,\n",
    "                       val_dataloader: torch.utils.data.DataLoader,\n",
    "                       loss_fn,\n",
    "                       device: torch.device,\n",
    "                       f1score: Callable,\n",
    "                       accuracy: Callable,\n",
    "                       DEBUG: bool = False) -> tuple:\n",
    "    \"\"\"\n",
    "        Function to validate the model for one epoch.\n",
    "\n",
    "        Parameters:\n",
    "        model: The model to validate.\n",
    "        val_dataloader: Dataloader for the validation set.\n",
    "        loss_fn: Loss function used for validation.\n",
    "        device: The device type used (GPU or CPU).\n",
    "        f1score: Function to calculate the F1 score.\n",
    "        accuracy: Function to calculate the accuracy.\n",
    "        DEBUG: A boolean flag used for debugging. If true, only 10 batches will be validated.\n",
    "\n",
    "        Returns:\n",
    "        A tuple containing average validation loss, accuracy, and F1 score for the epoch.\n",
    "    \"\"\"\n",
    "    # Switch model to evaluation mode\n",
    "    model.eval()\n",
    "    # Initialize counters for total validation loss, accuracy and F1 score\n",
    "    val_acc_total, val_loss_total, val_f1score_total = 0, 0, 0\n",
    "    # Disabling gradient calculation as we are in validation mode\n",
    "    with torch.no_grad():\n",
    "        # Loop through all batches in the validation dataloader\n",
    "        for batch_idx, (X, y) in enumerate(val_dataloader):\n",
    "            # Debug condition\n",
    "            if DEBUG and batch_idx == 10:\n",
    "                break\n",
    "            # Move the batch tensors to the same device as the model\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Forward pass\n",
    "            y_pred = model(X)\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            # Calculate the accuracy and F1-score\n",
    "            y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            f1score_metrics = f1score(y_pred_class, y)\n",
    "            acc = accuracy(y_pred_class, y)\n",
    "            # Update the running totals\n",
    "            val_loss_total = val_loss_total + loss.item()\n",
    "            val_acc_total = val_acc_total + acc\n",
    "            val_f1score_total = val_f1score_total + f1score_metrics\n",
    "            # Print a progress message\n",
    "            msg = f\"Batch: {batch_idx + 1} | Validation Loss: {loss.item():.4f} | Validation Accuracy: {acc:.4f} | Validation F1Score: {f1score_metrics:.4f}\"\n",
    "            print('\\r' + msg, end='', flush=True)\n",
    "        # Calculate the average loss, accuracy and F1-score\n",
    "        val_loss_avg = val_loss_total / len(val_dataloader)\n",
    "        val_acc_avg = val_acc_total / len(val_dataloader)\n",
    "        val_f1score_avg = val_f1score_total / len(val_dataloader)\n",
    "        # If DEBUG is enabled, calculate the average loss, accuracy and F1-score for the first 10 batches.\n",
    "        if DEBUG:\n",
    "            val_loss_avg = val_loss_total / 10\n",
    "            val_acc_avg = val_acc_total / 10\n",
    "            val_f1score_avg = val_f1score_total / 10\n",
    "        epoch_history = f\"Validation Loss: {val_loss_avg:.4f} | Validation Accuracy: {val_acc_avg:.4f} | Validation F1Score: {val_f1score_avg:.4f}\"\n",
    "        print('\\r' + epoch_history, end='\\n', flush=True)\n",
    "    return val_loss_avg, val_acc_avg, val_f1score_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, validate_one_epoch\n",
    "import torch\n",
    "from dataloaders import get_train_val_dataloader\n",
    "from models import EfficientNet\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "from config import get_args\n",
    "import time\n",
    "from utils import save_model\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from discordutils import send_msg\n",
    "from wandb_utils import log_values, initialize_wandb_run\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    DEBUG = args.DEBUG\n",
    "    if DEBUG:\n",
    "        print(\"Debugging mode is on. Only 10 batches will be used for training and validation.\")\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    f1score = F1Score(task=\"multiclass\", num_classes=25, average=\"macro\").to(args.device)\n",
    "    accuracy = Accuracy(task='multiclass', num_classes=25).to(args.device)\n",
    "    model = EfficientNet()\n",
    "    # model = torch.compile(model)\n",
    "    train_dataloader, _, _ = get_train_val_dataloader(root_dir=args.data_dir,\n",
    "                                                                         batch_size=args.batch_size)\n",
    "    model = model.to(args.device)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=args.learning_rate, steps_per_epoch=len(\n",
    "        train_dataloader), epochs=args.epochs)\n",
    "    logs = train(args, args.device, loss, f1score, accuracy, model, optimizer, scheduler=scheduler)\n",
    "    print(logs)\n",
    "\n",
    "\n",
    "def train(args, device, loss, f1score, accuracy, model, optimizer, starting_epoch=0, scheduler=None):\n",
    "    if args.wandb:\n",
    "        run = initialize_wandb_run(args)\n",
    "    print(\"Using train-test split.\")\n",
    "    DEBUG = args.DEBUG\n",
    "    train_loss_list, train_acc_list, train_f1score_list = [], [], []\n",
    "    val_loss_list, val_acc_list, val_f1score_list = [], [], []\n",
    "    train_dataloader, val_dataloader, classes = get_train_val_dataloader(root_dir=args.data_dir,\n",
    "                                                                         batch_size=args.batch_size)\n",
    "    if args.wandb:\n",
    "        run.watch(model, log=\"gradients\", log_freq=100)\n",
    "    for epoch in tqdm(range(starting_epoch, args.epochs + starting_epoch), disable=False, desc=\"Epochs\"):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nEpoch: {epoch}\")\n",
    "        epoch_loss, epoch_acc, epoch_f1score = train_one_epoch(model, train_dataloader, loss, optimizer, scheduler, device,\n",
    "                                                               f1score, accuracy, DEBUG)\n",
    "        epoch_val_loss, epoch_val_acc, epoch_val_f1score = validate_one_epoch(model, val_dataloader, loss, device,\n",
    "                                                                              f1score, accuracy, DEBUG)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        print(f\"Epoch time: {epoch_time:.2f} seconds\")\n",
    "        if args.wandb:\n",
    "            log_values(run, step=epoch, loss=epoch_loss, f1score=epoch_f1score, accuracy=epoch_acc, val_loss=epoch_val_loss, val_f1score=epoch_val_f1score, val_accuracy=epoch_val_acc)\n",
    "        train_loss_list.append(epoch_loss)\n",
    "        train_acc_list.append(epoch_acc)\n",
    "        train_f1score_list.append(epoch_f1score)\n",
    "        val_loss_list.append(epoch_val_loss)\n",
    "        val_acc_list.append(epoch_val_acc)\n",
    "        val_f1score_list.append(epoch_val_f1score)\n",
    "        # run.alert(title=\"Epoch completed\", text=f\"Epoch {epoch + 1} completed with F1score: {epoch_val_f1score:.4f}.\")\n",
    "        if args.discord:\n",
    "            send_msg(f\"Epoch {epoch} completed with F1score: {epoch_val_f1score:.4f}.\")\n",
    "        save_model(model_name=\"efficientnet\",\n",
    "                   model=model,\n",
    "                   optimizer=optimizer,\n",
    "                   epoch=epoch,\n",
    "                   loss=epoch_loss,\n",
    "                   val_f1score=epoch_val_f1score,\n",
    "                   directory=args.model_dir)\n",
    "    if args.wandb:\n",
    "        run.finish()\n",
    "    if args.discord:\n",
    "        send_msg(\"Training complete\")\n",
    "    return train_loss_list, train_acc_list, train_f1score_list, val_loss_list, val_acc_list, val_f1score_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
