{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ktrain\n",
      "  Downloading ktrain-0.39.0.tar.gz (25.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from ktrain) (1.3.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from ktrain) (3.8.1)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from ktrain) (2.1.3)\n",
      "Collecting fastprogress>=0.1.21 (from ktrain)\n",
      "  Downloading fastprogress-1.0.3-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: requests in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from ktrain) (2.31.0)\n",
      "Requirement already satisfied: joblib in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from ktrain) (1.3.2)\n",
      "Requirement already satisfied: packaging in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from ktrain) (23.2)\n",
      "Collecting langdetect (from ktrain)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jieba (from ktrain)\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from ktrain) (3.3.2)\n",
      "Requirement already satisfied: chardet in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from ktrain) (5.2.0)\n",
      "Collecting syntok>1.3.3 (from ktrain)\n",
      "  Downloading syntok-1.4.4-py3-none-any.whl (24 kB)\n",
      "Collecting tika (from ktrain)\n",
      "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.17.0 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from ktrain) (4.36.0)\n",
      "Requirement already satisfied: sentencepiece in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from ktrain) (0.1.99)\n",
      "Collecting keras_bert>=0.86.0 (from ktrain)\n",
      "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting whoosh (from ktrain)\n",
      "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from keras_bert>=0.86.0->ktrain) (1.26.0)\n",
      "Collecting keras-transformer==0.40.0 (from keras_bert>=0.86.0->ktrain)\n",
      "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-pos-embd==0.13.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
      "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-multi-head==0.29.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
      "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-layer-normalization==0.16.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
      "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-position-wise-feed-forward==0.8.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
      "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-embed-sim==0.10.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
      "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-self-attention==0.51.0 (from keras-multi-head==0.29.0->keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
      "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from matplotlib>=3.0.0->ktrain) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from matplotlib>=3.0.0->ktrain) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from matplotlib>=3.0.0->ktrain) (4.44.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from matplotlib>=3.0.0->ktrain) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from matplotlib>=3.0.0->ktrain) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from matplotlib>=3.0.0->ktrain) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from pandas>=1.0.1->ktrain) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from pandas>=1.0.1->ktrain) (2023.3)\n",
      "Requirement already satisfied: regex>2016 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from syntok>1.3.3->ktrain) (2023.10.3)\n",
      "Requirement already satisfied: filelock in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from transformers>=4.17.0->ktrain) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from transformers>=4.17.0->ktrain) (0.19.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from transformers>=4.17.0->ktrain) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from transformers>=4.17.0->ktrain) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from transformers>=4.17.0->ktrain) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from transformers>=4.17.0->ktrain) (4.66.1)\n",
      "Requirement already satisfied: six in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from langdetect->ktrain) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from requests->ktrain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from requests->ktrain) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from requests->ktrain) (2023.7.22)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from scikit-learn->ktrain) (1.11.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from scikit-learn->ktrain) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from tika->ktrain) (68.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.17.0->ktrain) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.17.0->ktrain) (4.8.0)\n",
      "Building wheels for collected packages: ktrain, keras_bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, jieba, langdetect, tika\n",
      "  Building wheel for ktrain (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ktrain: filename=ktrain-0.39.0-py3-none-any.whl size=25319738 sha256=7277907aaa41083d2f8593b2dd8d0d7834780f87fcde204724797534b8682fff\n",
      "  Stored in directory: /home/dai/.cache/pip/wheels/76/85/71/e3a5681a9980f88c5f7f89f4982ea63e5b1fdaecc5694a202e\n",
      "  Building wheel for keras_bert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras_bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33499 sha256=bc1dccedb0481fa7adc23d9ff07842eafa26dfbfd06c42cd25a0272b127903e5\n",
      "  Stored in directory: /home/dai/.cache/pip/wheels/89/dd/7a/e3c94de9a5b91bf45958aad780cadae9212a60c0dacd4576a4\n",
      "  Building wheel for keras-transformer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12285 sha256=b4379242ed057b3c1e814da1fb57db3d831db6e1844d26a186d265deff0db3b9\n",
      "  Stored in directory: /home/dai/.cache/pip/wheels/20/f6/8d/9063ab182cbf213b859ee9dfd602df75928007bbb916e8a2c7\n",
      "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3945 sha256=32ba202f63a361b516ad88a09e3e60146146be7c1a30650ef36b0909e4c5638e\n",
      "  Stored in directory: /home/dai/.cache/pip/wheels/58/0e/e7/55564ff6deabde27c81e86b26e2999ae83669683bc88fa83de\n",
      "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4654 sha256=d3b65f3d801ee4f634316ab64d74e38d2e5f34f05a869ce5defed4e23e94d22f\n",
      "  Stored in directory: /home/dai/.cache/pip/wheels/0f/c4/a1/24f1ca7fd39e75f4d8dab7feda6fe3e2163d8062b29f1169fb\n",
      "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14977 sha256=9ed33901c4c4eb43e04c26526bfefff54668644c84c6fbe4e0b6fe0512f90205\n",
      "  Stored in directory: /home/dai/.cache/pip/wheels/19/fe/36/14fd933930357c34331e65e11aa5380e35188df2223b113358\n",
      "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6945 sha256=89670554b203d01237eca022c43a025b102b8a91607bcbfcc48046247fe930de\n",
      "  Stored in directory: /home/dai/.cache/pip/wheels/f3/b3/f2/9acf9a5c6b16a27d3e9080901f49ae2774dcaca251a752d82a\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4969 sha256=b85da8b72feb1ca386d2593bb13aac638be279144ac9abfb91684fd665c82092\n",
      "  Stored in directory: /home/dai/.cache/pip/wheels/af/32/fe/0dad05357a0f660b6bf2dbafcfda95bc411b1f51f71a071162\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18895 sha256=56417c7c534c2b82c03d49d4f9423bf29bbe66470ab169e025751c5d074bfc2b\n",
      "  Stored in directory: /home/dai/.cache/pip/wheels/46/f9/96/709295c836133071c12a300729fed4027757f889c01695feea\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=12c67e894a2d2dc1e529e2d4e31e90a5fbe4b2275a55381adfaefb5ab5991406\n",
      "  Stored in directory: /home/dai/.cache/pip/wheels/ac/60/cf/538a1f183409caf1fc136b5d2c2dee329001ef6da2c5084bef\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=3f5b3df3176770a8b7ec4ae1d592731ccb6b3d6477617ac815a006504aef67e2\n",
      "  Stored in directory: /home/dai/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "  Building wheel for tika (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32621 sha256=0fcac8606e6712ab8702349fb84af36238c2c6137dd81018b8c6e01679f6f416\n",
      "  Stored in directory: /home/dai/.cache/pip/wheels/27/ba/2f/37420d1191bdae5e855d69b8e913673045bfd395cbd78ad697\n",
      "Successfully built ktrain keras_bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention jieba langdetect tika\n",
      "Installing collected packages: whoosh, jieba, syntok, langdetect, keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-layer-normalization, keras-embed-sim, fastprogress, tika, keras-multi-head, keras-transformer, keras_bert, ktrain\n",
      "Successfully installed fastprogress-1.0.3 jieba-0.42.1 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 keras_bert-0.89.0 ktrain-0.39.0 langdetect-1.0.9 syntok-1.4.4 tika-2.6.0 whoosh-2.7.4\n"
     ]
    }
   ],
   "source": [
    "!pip install ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 12:30:54.483419: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-22 12:30:54.545780: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-22 12:30:54.545822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-22 12:30:54.547197: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-22 12:30:54.556688: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-22 12:30:55.731131: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./SMSSpamCollection', sep='\\t', names=['label', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'spam']\n",
      "      ham  spam\n",
      "1114  1.0   0.0\n",
      "3589  1.0   0.0\n",
      "3095  1.0   0.0\n",
      "1012  1.0   0.0\n",
      "3320  1.0   0.0\n",
      "['ham', 'spam']\n",
      "      ham  spam\n",
      "4456  1.0   0.0\n",
      "690   0.0   1.0\n",
      "944   1.0   0.0\n",
      "3768  1.0   0.0\n",
      "1189  1.0   0.0\n",
      "downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n",
      "[██████████████████████████████████████████████████]\n",
      "extracting pretrained BERT model...\n",
      "done.\n",
      "\n",
      "cleanup downloaded zip...\n",
      "done.\n",
      "\n",
      "preprocessing train...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_df(train_df=df_train,\n",
    "                                                                   text_column='text',\n",
    "                                                                   label_columns='label',\n",
    "                                                                   val_df=df_test,\n",
    "                                                                   preprocess_mode='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "maxlen is 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "model = text.text_classifier(name='bert',\n",
    "                                train_data=(x_train, y_train),\n",
    "                                preproc=preproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ktrain.get_learner(model=model, train_data=(x_train, y_train), val_data=(x_test, y_test), batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulating training for different learning rates... this may take a few moments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "learner.lr_find(max_epochs=3, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_onecycle(lr=2e-3, epochs=1)\n",
    "predictor = ktrain.get_predictor(learner.model, preproc)\n",
    "predictor.save('bert-spam-classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg1 = ['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...']\n",
    "msg2 = ['Thanks for your subscription to Ringtone UK your mobile will be charged £5/month Please confirm by replying YES or NO. If you reply NO you will not be charged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(msg1)\n",
    "predictor.predict(msg2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
