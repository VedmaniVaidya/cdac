{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 12:28:42.294851: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-18 12:28:42.356362: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-18 12:28:42.356402: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-18 12:28:42.357769: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-18 12:28:42.367166: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-18 12:28:43.557042: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.3-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: absl-py in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from tensorflow_datasets) (2.0.0)\n",
      "Collecting array-record (from tensorflow_datasets)\n",
      "  Downloading array_record-0.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (503 bytes)\n",
      "Requirement already satisfied: click in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Downloading dm_tree-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m804.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting etils>=0.9.0 (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets)\n",
      "  Downloading etils-1.6.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from tensorflow_datasets) (1.26.0)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from tensorflow_datasets) (4.23.4)\n",
      "Requirement already satisfied: psutil in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from tensorflow_datasets) (5.9.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from tensorflow_datasets) (2.31.0)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: termcolor in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from tensorflow_datasets) (2.3.0)\n",
      "Collecting toml (from tensorflow_datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: tqdm in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from tensorflow_datasets) (4.66.1)\n",
      "Requirement already satisfied: wrapt in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Requirement already satisfied: fsspec in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (2023.10.0)\n",
      "Collecting importlib_resources (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets)\n",
      "  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: typing_extensions in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.8.0)\n",
      "Collecting zipp (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets)\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (2023.7.22)\n",
      "Requirement already satisfied: six in /home/dai/anaconda3/envs/dnn/lib/python3.11/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Collecting absl-py (from tensorflow_datasets)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting googleapis-common-protos<2,>=1.52.0 (from tensorflow-metadata->tensorflow_datasets)\n",
      "  Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf>=3.20 (from tensorflow_datasets)\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_datasets-4.9.3-py3-none-any.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading etils-1.6.0-py3-none-any.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.9/144.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading array_record-0.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
      "Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.7/228.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21484 sha256=322b9b49b3787c3b0391c34beb3bb9b7d099cd064fc42add601ee344d874f005\n",
      "  Stored in directory: /home/dai/.cache/pip/wheels/90/74/b1/9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, zipp, toml, protobuf, promise, importlib_resources, etils, absl-py, googleapis-common-protos, tensorflow-metadata, array-record, tensorflow_datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.4\n",
      "    Uninstalling protobuf-4.23.4:\n",
      "      Successfully uninstalled protobuf-4.23.4\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 2.0.0\n",
      "    Uninstalling absl-py-2.0.0:\n",
      "      Successfully uninstalled absl-py-2.0.0\n",
      "Successfully installed absl-py-1.4.0 array-record-0.5.0 dm-tree-0.1.8 etils-1.6.0 googleapis-common-protos-1.62.0 importlib_resources-6.1.1 promise-2.3 protobuf-3.20.3 tensorflow-metadata-1.14.0 tensorflow_datasets-4.9.3 toml-0.10.2 zipp-3.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb,info = tfds.load(\"imdb_reviews\",with_info=True,as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentence = []\n",
    "training_labels = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "max_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import pad_sequence\n",
    "Tokenizer = Tokenizer(num_words=vocab_size)\n",
    "Tokenizer.fit_on_texts(training_labels)\n",
    "word_index = Tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = Tokenizer.text_to_sequences(training_sentence)\n",
    "padded = pad_sequence(sequences,maxlen=max,truncating='pre')\n",
    "testing_sequence = Tokenizer.texts_to_sequences(testing_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
